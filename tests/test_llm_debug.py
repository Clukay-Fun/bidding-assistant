"""
LLM å“åº”è°ƒè¯•è„šæœ¬
ç”¨äºæŸ¥çœ‹ LLM å®é™…è¿”å›çš„å†…å®¹æ ¼å¼
"""

import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

from openai import OpenAI
from app.config import (
    SILICONFLOW_API_KEY,
    SILICONFLOW_BASE_URL,
    REASONING_MODEL,
)


def test_llm_response():
    """æµ‹è¯• LLM åŸå§‹å“åº”"""
    print("\n" + "=" * 50)
    print("ğŸ§ª LLM å“åº”è°ƒè¯•")
    print("=" * 50)
    
    client = OpenAI(
        api_key=SILICONFLOW_API_KEY,
        base_url=SILICONFLOW_BASE_URL,
    )
    
    # ç®€å•çš„æµ‹è¯•æç¤ºè¯
    prompt = """
ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚è¯·ç”¨ä»¥ä¸‹ JSON æ ¼å¼å›å¤ï¼š
```json
{
    "thought": "ä½ çš„æ€è€ƒ",
    "action": null,
    "answer": "ä½ çš„å›ç­”"
}
```

ç”¨æˆ·é—®é¢˜ï¼šä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±

è¯·è¾“å‡º JSONï¼š
"""
    
    print(f"\nğŸ“¤ å‘é€è¯·æ±‚åˆ°: {REASONING_MODEL}")
    
    response = client.chat.completions.create(
        model=REASONING_MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1,
        max_tokens=1000,
    )
    
    content = response.choices[0].message.content
    
    print(f"\nğŸ“¨ åŸå§‹å“åº” (repr):")
    print(repr(content))
    
    print(f"\nğŸ“¨ åŸå§‹å“åº” (æ˜¾ç¤º):")
    print(content)
    
    print(f"\nğŸ“Š å“åº”é•¿åº¦: {len(content)} å­—ç¬¦")


if __name__ == "__main__":
    test_llm_response()