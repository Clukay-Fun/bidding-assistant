"""
AIé©±åŠ¨çš„æ–‡æ¡£ç»“æ„è§£æå™¨
åŠŸèƒ½ï¼šä½¿ç”¨å¤§æ¨¡å‹å°†Markdownæ–‡æ¡£è§£æä¸ºå±‚çº§Nodeç»“æ„
"""

from pathlib import Path
from openai import OpenAI
import json
import time
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import (
    SILICONFLOW_API_KEY, 
    SILICONFLOW_BASE_URL, 
    EXTRACT_MODEL,
    OUTPUT_DIR
)
from src.utils import load_prompt, clean_json_response


# ============================================
# region APIå®¢æˆ·ç«¯
# ============================================

def get_client() -> OpenAI:
    """è·å–APIå®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=SILICONFLOW_API_KEY,
        base_url=SILICONFLOW_BASE_URL
    )

# endregion
# ============================================


# ============================================
# region ç»“æ„åŒ–è§£æ
# ============================================

def parse_document_structure(markdown_content: str, chunk_size: int = 15000) -> dict:
    """
    ä½¿ç”¨AIè§£ææ–‡æ¡£ç»“æ„
    
    å‚æ•°:
        markdown_content: Markdownæ–‡æ¡£å†…å®¹
        chunk_size: åˆ†å—å¤§å°ï¼ˆé¿å…è¶…å‡ºtokené™åˆ¶ï¼‰
    
    è¿”å›:
        å±‚çº§ç»“æ„å­—å…¸
    """
    client = get_client()
    
    # å¦‚æœæ–‡æ¡£è¾ƒçŸ­ï¼Œç›´æ¥è§£æ
    if len(markdown_content) <= chunk_size:
        return _parse_chunk(client, markdown_content)
    
    # æ–‡æ¡£è¾ƒé•¿ï¼Œåˆ†å—è§£æååˆå¹¶
    print(f"ğŸ“„ æ–‡æ¡£è¾ƒé•¿ï¼ˆ{len(markdown_content)}å­—ç¬¦ï¼‰ï¼Œåˆ†å—å¤„ç†...")
    chunks = _split_by_chapters(markdown_content)
    
    all_children = []
    root_content = ""
    
    for i, chunk in enumerate(chunks):
        print(f"   ğŸ” è§£æç¬¬ {i+1}/{len(chunks)} å—...")
        result = _parse_chunk(client, chunk)
        
        if result:
            # æ”¶é›†æ ¹èŠ‚ç‚¹å†…å®¹
            if result.get("content"):
                root_content += result["content"] + "\n"
            
            # æ”¶é›†å­èŠ‚ç‚¹
            if result.get("children"):
                all_children.extend(result["children"])
    
    return {
        "title": "æ‹›æ ‡æ–‡ä»¶",
        "level": 0,
        "content": root_content.strip(),
        "children": all_children
    }


def _parse_chunk(client: OpenAI, content: str) -> dict:
    """è§£æå•ä¸ªæ–‡æ¡£å—"""
    try:
        # åŠ è½½æç¤ºè¯
        prompt_template = load_prompt("structure_parse")
        prompt = prompt_template.replace("{document_content}", content)
        
        response = client.chat.completions.create(
            model=EXTRACT_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚"
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            temperature=0.1,
            max_tokens=16000
        )
        
        result_text = response.choices[0].message.content.strip()
        result_text = clean_json_response(result_text)
        
        return json.loads(result_text)
        
    except json.JSONDecodeError as e:
        print(f"   âš ï¸ JSONè§£æå¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"   âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return None


def _split_by_chapters(content: str) -> list:
    """æŒ‰ç« èŠ‚åˆ†å‰²æ–‡æ¡£"""
    import re
    
    # æŒ‰"ç¬¬Xç« "åˆ†å‰²
    pattern = r'(?=^#*\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )'
    chunks = re.split(pattern, content, flags=re.MULTILINE)
    
    # è¿‡æ»¤ç©ºå—
    chunks = [c.strip() for c in chunks if c.strip()]
    
    # å¦‚æœæ²¡æœ‰ç« èŠ‚æ ‡è®°ï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
    if len(chunks) <= 1:
        chunk_size = 12000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    return chunks

# endregion
# ============================================


# ============================================
# region è½¬æ¢ä¸ºNodeåˆ—è¡¨
# ============================================

def structure_to_nodes(structure: dict, parent_id: str = None) -> list:
    """
    å°†å±‚çº§ç»“æ„è½¬æ¢ä¸ºæ‰å¹³çš„Nodeåˆ—è¡¨
    
    æ¯ä¸ªNodeåŒ…å«:
        - id: å”¯ä¸€æ ‡è¯†
        - text: å†…å®¹æ–‡æœ¬
        - metadata: å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€å±‚çº§ã€çˆ¶èŠ‚ç‚¹ç­‰ï¼‰
    """
    nodes = []
    
    def _traverse(node: dict, parent_id: str = None, path: list = []):
        # ç”ŸæˆèŠ‚ç‚¹ID
        node_id = f"node_{len(nodes)}"
        
        # æ„å»ºå½“å‰è·¯å¾„
        current_path = path + [node.get("title", "")]
        
        # åˆ›å»ºNode
        node_data = {
            "id": node_id,
            "text": node.get("content", ""),
            "metadata": {
                "title": node.get("title", ""),
                "level": node.get("level", 0),
                "parent_id": parent_id,
                "path": " > ".join(current_path),
                "has_children": len(node.get("children", [])) > 0
            }
        }
        
        nodes.append(node_data)
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.get("children", []):
            _traverse(child, node_id, current_path)
    
    _traverse(structure)
    return nodes

# endregion
# ============================================


# ============================================
# region ä¸»å‡½æ•°
# ============================================

def parse_markdown_to_nodes(markdown_path: str) -> list:
    """
    è§£æMarkdownæ–‡ä»¶ä¸ºNodeåˆ—è¡¨
    
    å‚æ•°:
        markdown_path: Markdownæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        Nodeåˆ—è¡¨
    """
    start_time = time.time()
    print(f"\n{'='*50}")
    print(f"ğŸš€ å¼€å§‹è§£ææ–‡æ¡£ç»“æ„")
    print(f"{'='*50}")
    print(f"ğŸ“„ æ–‡ä»¶: {markdown_path}")
    
    # è¯»å–æ–‡ä»¶
    with open(markdown_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"ğŸ“ æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
    
    # AIè§£æç»“æ„
    print(f"\nğŸ¤– è°ƒç”¨AIè§£ææ–‡æ¡£ç»“æ„...")
    structure = parse_document_structure(content)
    
    if not structure:
        print("âŒ æ–‡æ¡£ç»“æ„è§£æå¤±è´¥")
        return []
    
    # ä¿å­˜ç»“æ„ï¼ˆè°ƒè¯•ç”¨ï¼‰
    structure_path = OUTPUT_DIR / f"{Path(markdown_path).stem}_structure.json"
    with open(structure_path, 'w', encoding='utf-8') as f:
        json.dump(structure, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æ„å·²ä¿å­˜: {structure_path}")
    
    # è½¬æ¢ä¸ºNodeåˆ—è¡¨
    nodes = structure_to_nodes(structure)
    
    # ä¿å­˜Nodeåˆ—è¡¨
    nodes_path = OUTPUT_DIR / f"{Path(markdown_path).stem}_nodes.json"
    with open(nodes_path, 'w', encoding='utf-8') as f:
        json.dump(nodes, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Nodeså·²ä¿å­˜: {nodes_path}")
    
    elapsed = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… è§£æå®Œæˆ!")
    print(f"ğŸ“Š å…±ç”Ÿæˆ {len(nodes)} ä¸ªNode")
    print(f"â±ï¸ è€—æ—¶: {elapsed:.2f} ç§’")
    print(f"{'='*50}")
    
    return nodes

# endregion
# ============================================


if __name__ == "__main__":
    if len(sys.argv) > 1:
        parse_markdown_to_nodes(sys.argv[1])
    else:
        # é»˜è®¤æµ‹è¯•
        test_file = OUTPUT_DIR / "é‡‡è´­æ–‡ä»¶.md"
        if test_file.exists():
            parse_markdown_to_nodes(str(test_file))
        else:
            print("ç”¨æ³•: python -m src.node_parser <Markdownæ–‡ä»¶è·¯å¾„>")
