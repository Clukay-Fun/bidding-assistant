"""
RAGé—®ç­”ç³»ç»Ÿ
åŠŸèƒ½ï¼šæ£€ç´¢ç›¸å…³æ–‡æ¡£ + LLMç”Ÿæˆç­”æ¡ˆï¼ˆå¸¦å¼•ç”¨ï¼‰
"""

from pathlib import Path
from openai import OpenAI as OpenAIClient
import httpx
import time
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from llama_index.core import Settings, VectorStoreIndex
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from qdrant_client import QdrantClient

from config.settings import (
    SILICONFLOW_API_KEY,
    SILICONFLOW_BASE_URL,
    EMBEDDING_MODEL,
    EMBEDDING_BATCH_SIZE,
    REASONING_MODEL,
    RERANK_MODEL,
    QDRANT_PATH,
    QDRANT_COLLECTION_NAME
)
from src.utils import load_prompt


# ============================================
# region åˆå§‹åŒ–
# ============================================

def get_llm_client() -> OpenAIClient:
    """è·å–LLMå®¢æˆ·ç«¯"""
    return OpenAIClient(
        api_key=SILICONFLOW_API_KEY,
        base_url=SILICONFLOW_BASE_URL
    )


def init_embedding():
    """åˆå§‹åŒ–Embeddingæ¨¡å‹"""
    embed_model = OpenAIEmbedding(
        api_key=SILICONFLOW_API_KEY,
        api_base=SILICONFLOW_BASE_URL,
        model_name=EMBEDDING_MODEL,
        embed_batch_size=EMBEDDING_BATCH_SIZE,
    )
    Settings.embed_model = embed_model
    print(f"âœ… Embedding: {EMBEDDING_MODEL}")


def load_index() -> VectorStoreIndex:
    """åŠ è½½å·²æœ‰çš„å‘é‡ç´¢å¼•"""
    print(f"ğŸ“‚ åŠ è½½ç´¢å¼•: {QDRANT_COLLECTION_NAME}")
    
    client = QdrantClient(path=QDRANT_PATH)
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=QDRANT_COLLECTION_NAME,
    )
    
    index = VectorStoreIndex.from_vector_store(vector_store)
    print(f"âœ… ç´¢å¼•åŠ è½½å®Œæˆ")
    return index

# endregion
# ============================================


# ============================================
# region Reranké‡æ’åº
# ============================================

def rerank_nodes(query: str, nodes: list, top_n: int = 3) -> list:
    """
    ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„Rerank APIå¯¹ç»“æœé‡æ’åº
    """
    if not nodes:
        return nodes
    
    try:
        documents = [node.text for node in nodes]
        
        response = httpx.post(
            f"{SILICONFLOW_BASE_URL}/rerank",
            headers={
                "Authorization": f"Bearer {SILICONFLOW_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": RERANK_MODEL,
                "query": query,
                "documents": documents,
                "top_n": top_n
            },
            timeout=30.0
        )
        
        if response.status_code == 200:
            result = response.json()
            
            reranked_nodes = []
            for item in result.get("results", []):
                idx = item["index"]
                if idx < len(nodes):
                    nodes[idx].score = item["relevance_score"]
                    reranked_nodes.append(nodes[idx])
            
            print(f"   ğŸ”„ Rerankå®Œæˆï¼Œä¿ç•™ {len(reranked_nodes)} ä¸ªç»“æœ")
            return reranked_nodes
        else:
            print(f"   âš ï¸ Rerank APIè¿”å›é”™è¯¯: {response.status_code}")
            return nodes[:top_n]
            
    except Exception as e:
        print(f"   âš ï¸ Rerankå¤±è´¥: {e}")
        return nodes[:top_n]

# endregion
# ============================================


# ============================================
# region LLMè°ƒç”¨
# ============================================

def call_llm(prompt: str) -> str:
    """è°ƒç”¨LLMç”Ÿæˆå›ç­”"""
    client = get_llm_client()
    
    response = client.chat.completions.create(
        model=REASONING_MODEL,
        messages=[
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ‹›æŠ•æ ‡æ–‡æ¡£åˆ†æåŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        temperature=0.1,
        max_tokens=2000
    )
    
    return response.choices[0].message.content

# endregion
# ============================================


# ============================================
# region RAGé—®ç­”
# ============================================

def query_with_sources(
    index: VectorStoreIndex,
    question: str,
    top_k: int = 5,
    use_rerank: bool = True,
    rerank_top_n: int = 3
) -> dict:
    """
    å¸¦å¼•ç”¨æ¥æºçš„RAGé—®ç­”
    """
    start_time = time.time()
    
    # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
    retriever = index.as_retriever(similarity_top_k=top_k)
    nodes = retriever.retrieve(question)
    print(f"   ğŸ“„ æ£€ç´¢åˆ° {len(nodes)} ä¸ªç›¸å…³æ–‡æ¡£")
    
    # 2. é‡æ’åº
    if use_rerank and len(nodes) > 0:
        nodes = rerank_nodes(question, nodes, top_n=rerank_top_n)
    
    # 3. æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    sources = []
    
    for i, node in enumerate(nodes, 1):
        title = node.metadata.get("title", "æœªçŸ¥")
        path = node.metadata.get("path", "æœªçŸ¥")
        content = node.text
        
        context_parts.append(f"ã€æ–‡æ¡£{i}ã€‘\næ ‡é¢˜: {title}\nè·¯å¾„: {path}\nå†…å®¹: {content}\n")
        sources.append({
            "title": title,
            "path": path,
            "content": content[:200] + "..." if len(content) > 200 else content,
            "score": node.score
        })
    
    context = "\n".join(context_parts)
    
    # 4. åŠ è½½æç¤ºè¯å¹¶è°ƒç”¨LLM
    prompt_template = load_prompt("rag_qa")
    prompt = prompt_template.replace("{context}", context).replace("{question}", question)
    answer = call_llm(prompt)
    
    elapsed = time.time() - start_time
    
    return {
        "answer": answer,
        "sources": sources,
        "time": elapsed
    }

# endregion
# ============================================


# ============================================
# region äº¤äº’å¼é—®ç­”
# ============================================

def interactive_qa(index: VectorStoreIndex):
    """äº¤äº’å¼é—®ç­”"""
    print("\n" + "="*50)
    print("ğŸ’¬ æ‹›æŠ•æ ‡æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    print("="*50)
    print("è¾“å…¥é—®é¢˜è¿›è¡ŒæŸ¥è¯¢ï¼Œè¾“å…¥ 'quit' æˆ– 'q' é€€å‡º\n")
    
    while True:
        question = input("ğŸ™‹ ä½ çš„é—®é¢˜: ").strip()
        
        if question.lower() in ['quit', 'q', 'é€€å‡º']:
            print("ğŸ‘‹ å†è§ï¼")
            break
        
        if not question:
            continue
        
        print(f"\nğŸ” æ­£åœ¨æŸ¥è¯¢...")
        
        try:
            result = query_with_sources(index, question)
            
            print(f"\n{'='*50}")
            print(f"ğŸ“ å›ç­”ï¼š")
            print(f"{'='*50}")
            print(result["answer"])
            
            print(f"\n{'='*50}")
            print(f"ğŸ“š å¼•ç”¨æ¥æºï¼ˆå…±{len(result['sources'])}æ¡ï¼‰ï¼š")
            print(f"{'='*50}")
            for i, source in enumerate(result["sources"], 1):
                print(f"\n[{i}] {source['title']}")
                print(f"    è·¯å¾„: {source['path']}")
                print(f"    ç›¸ä¼¼åº¦: {source['score']:.4f}")
            
            print(f"\nâ±ï¸ è€—æ—¶: {result['time']:.2f} ç§’\n")
            
        except Exception as e:
            print(f"âŒ æŸ¥è¯¢å‡ºé”™: {e}\n")

# endregion
# ============================================


# ============================================
# region ä¸»å‡½æ•°
# ============================================

def main():
    print("\n" + "="*50)
    print("ğŸš€ RAGé—®ç­”ç³»ç»Ÿå¯åŠ¨")
    print("="*50 + "\n")
    
    # åˆå§‹åŒ–
    init_embedding()
    print(f"âœ… LLM: {REASONING_MODEL}")
    print(f"âœ… Rerank: {RERANK_MODEL}")
    
    # åŠ è½½ç´¢å¼•
    index = load_index()
    
    # å¯åŠ¨äº¤äº’å¼é—®ç­”
    interactive_qa(index)


if __name__ == "__main__":
    main()
