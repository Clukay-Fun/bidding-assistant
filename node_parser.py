"""
AIé©±åŠ¨çš„æ–‡æ¡£ç»“æ„è§£æå™¨
åŠŸèƒ½ï¼šä½¿ç”¨å¤§æ¨¡å‹å°†Markdownæ–‡æ¡£è§£æä¸ºå±‚çº§Nodeç»“æ„
"""

from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv
import json
import os
import time

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ============================================
# region é…ç½®åŒºåŸŸ
# ============================================

# ç¡…åŸºæµåŠ¨APIé…ç½®
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"

# æ¨¡å‹é€‰æ‹©
STRUCTURE_MODEL = "Qwen/Qwen3-8B"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = Path("./output")
OUTPUT_DIR.mkdir(exist_ok=True)

# endregion
# ============================================


# ============================================
# region APIå®¢æˆ·ç«¯
# ============================================

def get_client() -> OpenAI:
    """è·å–APIå®¢æˆ·ç«¯"""
    return OpenAI(
        api_key=SILICONFLOW_API_KEY,
        base_url=SILICONFLOW_BASE_URL
    )

# endregion
# ============================================


# ============================================
# region ç»“æ„åŒ–è§£æ
# ============================================

STRUCTURE_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ï¼Œä¸“é—¨å¤„ç†æ‹›æŠ•æ ‡æ–‡æ¡£ã€‚

è¯·åˆ†æä»¥ä¸‹æ–‡æ¡£å†…å®¹ï¼Œå°†å…¶è§£æä¸ºå±‚çº§ç»“æ„ã€‚

## è§£æè§„åˆ™

1. **è¯†åˆ«æ ‡é¢˜å±‚çº§**ï¼š
   - å±‚çº§1ï¼šç« æ ‡é¢˜ï¼ˆå¦‚"ç¬¬ä¸€ç« "ã€"ç¬¬äºŒç« "ï¼‰
   - å±‚çº§2ï¼šèŠ‚æ ‡é¢˜ï¼ˆå¦‚"ä¸€ã€"ã€"äºŒã€"ï¼‰
   - å±‚çº§3ï¼šæ¡æ¬¾æ ‡é¢˜ï¼ˆå¦‚"ï¼ˆä¸€ï¼‰"ã€"ï¼ˆäºŒï¼‰"ï¼‰
   - å±‚çº§4ï¼šå­æ¡æ¬¾ï¼ˆå¦‚"1."ã€"2."æˆ–æ›´æ·±å±‚ç¼–å·ï¼‰

2. **æ¯ä¸ªèŠ‚ç‚¹åŒ…å«**ï¼š
   - title: æ ‡é¢˜æ–‡æœ¬
   - level: å±‚çº§æ•°å­—(1-4)
   - content: **ä»…è¯¥æ ‡é¢˜ä¸‹çš„ç›´æ¥æ­£æ–‡å†…å®¹ï¼Œä¸åŒ…å«ä»»ä½•å­æ ‡é¢˜åŠå…¶å†…å®¹**
   - children: å­èŠ‚ç‚¹æ•°ç»„

3. **é‡è¦ï¼šé¿å…å†…å®¹é‡å¤**ï¼š
   - å¦‚æœæ ‡é¢˜ä¸‹ç´§æ¥ç€å°±æ˜¯å­æ ‡é¢˜ï¼Œåˆ™contentä¸ºç©ºå­—ç¬¦ä¸²""
   - å­æ ‡é¢˜åŠå…¶å†…å®¹åªèƒ½å‡ºç°åœ¨childrenä¸­ï¼Œä¸èƒ½é‡å¤å‡ºç°åœ¨çˆ¶èŠ‚ç‚¹çš„contentä¸­

4. **æå…¶é‡è¦ï¼šä¿è¯å†…å®¹å®Œæ•´**ï¼š
   - **å¿…é¡»ä¿ç•™åŸæ–‡çš„æ¯ä¸€å¥è¯ã€æ¯ä¸€ä¸ªæ®µè½ï¼Œç»å¯¹ä¸èƒ½çœç•¥ä»»ä½•å†…å®¹**
   - **æ‰€æœ‰ç¼–å·é¡¹ï¼ˆ1ï¼‰2ï¼‰3ï¼‰4ï¼‰ç­‰ï¼‰å¿…é¡»å…¨éƒ¨ä¿ç•™ï¼Œä¸èƒ½é—æ¼**
   - **å¦‚æœå†…å®¹å¾ˆé•¿ï¼Œä¹Ÿå¿…é¡»å®Œæ•´è¾“å‡ºï¼Œä¸è¦æˆªæ–­æˆ–æ€»ç»“**
   - è¡¨æ ¼å†…å®¹å¿…é¡»å®Œæ•´ä¿ç•™
   - å®å¯è¾“å‡ºæ›´é•¿çš„JSONï¼Œä¹Ÿä¸èƒ½ä¸¢å¤±ä»»ä½•åŸæ–‡å†…å®¹

5. **æ³¨æ„äº‹é¡¹**ï¼š
   - è¡¨æ ¼å†…å®¹å½’å±äºå…¶ä¸Šæ–¹æœ€è¿‘çš„æ ‡é¢˜
   - æ²¡æœ‰æ ‡é¢˜çš„å¼€å¤´å†…å®¹å½’å±äºæ–‡æ¡£æ ¹èŠ‚ç‚¹

## è¾“å‡ºæ ¼å¼

è¯·ç›´æ¥è¾“å‡ºJSONï¼Œä¸è¦åŒ…å«```json```æ ‡è®°ï¼š
{
  "title": "æ–‡æ¡£æ ‡é¢˜",
  "level": 0,
  "content": "æ–‡æ¡£å¼€å¤´çš„éæ ‡é¢˜å†…å®¹ï¼ˆå¦‚æœç´§æ¥å­æ ‡é¢˜åˆ™ä¸ºç©ºï¼‰",
  "children": [
    {
      "title": "ç¬¬ä¸€ç«  XXX",
      "level": 1,
      "content": "",
      "children": [
        {
          "title": "ä¸€ã€XXX",
          "level": 2,
          "content": "è¯¥èŠ‚çš„å®Œæ•´æ­£æ–‡å†…å®¹ï¼Œå¿…é¡»åŒ…å«æ‰€æœ‰æ®µè½å’Œç¼–å·é¡¹...",
          "children": []
        }
      ]
    }
  ]
}

## å¾…è§£ææ–‡æ¡£

"""


def parse_document_structure(markdown_content: str, chunk_size: int = 15000) -> dict:
    """
    ä½¿ç”¨AIè§£ææ–‡æ¡£ç»“æ„
    
    å‚æ•°:
        markdown_content: Markdownæ–‡æ¡£å†…å®¹
        chunk_size: åˆ†å—å¤§å°ï¼ˆé¿å…è¶…å‡ºtokené™åˆ¶ï¼‰
    
    è¿”å›:
        å±‚çº§ç»“æ„å­—å…¸
    """
    client = get_client()
    
    # å¦‚æœæ–‡æ¡£è¾ƒçŸ­ï¼Œç›´æ¥è§£æ
    if len(markdown_content) <= chunk_size:
        return _parse_chunk(client, markdown_content)
    
    # æ–‡æ¡£è¾ƒé•¿ï¼Œåˆ†å—è§£æååˆå¹¶
    print(f"ğŸ“„ æ–‡æ¡£è¾ƒé•¿ï¼ˆ{len(markdown_content)}å­—ç¬¦ï¼‰ï¼Œåˆ†å—å¤„ç†...")
    chunks = _split_by_chapters(markdown_content)
    
    all_children = []
    root_content = ""
    
    for i, chunk in enumerate(chunks):
        print(f"   ğŸ” è§£æç¬¬ {i+1}/{len(chunks)} å—...")
        result = _parse_chunk(client, chunk)
        
        if result:
            # æ”¶é›†æ ¹èŠ‚ç‚¹å†…å®¹
            if result.get("content"):
                root_content += result["content"] + "\n"
            
            # æ”¶é›†å­èŠ‚ç‚¹
            if result.get("children"):
                all_children.extend(result["children"])
    
    return {
        "title": "æ‹›æ ‡æ–‡ä»¶",
        "level": 0,
        "content": root_content.strip(),
        "children": all_children
    }


def _parse_chunk(client: OpenAI, content: str) -> dict:
    """è§£æå•ä¸ªæ–‡æ¡£å—"""
    try:
        response = client.chat.completions.create(
            model=STRUCTURE_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æ¡£ç»“æ„åˆ†æä¸“å®¶ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–å†…å®¹ã€‚"
                },
                {
                    "role": "user", 
                    "content": STRUCTURE_PROMPT + content
                }
            ],
            temperature=0.1,  # ä½æ¸©åº¦ä¿è¯è¾“å‡ºç¨³å®š
            max_tokens=16000
        )
        
        result_text = response.choices[0].message.content.strip()
        
        # æ¸…ç†å¯èƒ½çš„markdownæ ‡è®°
        if result_text.startswith("```"):
            result_text = result_text.split("```")[1]
            if result_text.startswith("json"):
                result_text = result_text[4:]
        if result_text.endswith("```"):
            result_text = result_text[:-3]
        
        return json.loads(result_text.strip())
        
    except json.JSONDecodeError as e:
        print(f"   âš ï¸ JSONè§£æå¤±è´¥: {e}")
        print(f"   åŸå§‹è¾“å‡º: {result_text[:500]}...")
        return None
    except Exception as e:
        print(f"   âŒ APIè°ƒç”¨å¤±è´¥: {e}")
        return None


def _split_by_chapters(content: str) -> list:
    """æŒ‰ç« èŠ‚åˆ†å‰²æ–‡æ¡£"""
    import re
    
    # æŒ‰"ç¬¬Xç« "åˆ†å‰²
    pattern = r'(?=^#*\s*ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾]+ç« )'
    chunks = re.split(pattern, content, flags=re.MULTILINE)
    
    # è¿‡æ»¤ç©ºå—
    chunks = [c.strip() for c in chunks if c.strip()]
    
    # å¦‚æœæ²¡æœ‰ç« èŠ‚æ ‡è®°ï¼ŒæŒ‰å›ºå®šé•¿åº¦åˆ†å‰²
    if len(chunks) <= 1:
        chunk_size = 12000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
    
    return chunks

# endregion
# ============================================


# ============================================
# region è½¬æ¢ä¸ºLlamaIndex Node
# ============================================

def structure_to_nodes(structure: dict, parent_id: str = None) -> list:
    """
    å°†å±‚çº§ç»“æ„è½¬æ¢ä¸ºæ‰å¹³çš„Nodeåˆ—è¡¨
    
    æ¯ä¸ªNodeåŒ…å«:
        - id: å”¯ä¸€æ ‡è¯†
        - text: å†…å®¹æ–‡æœ¬
        - metadata: å…ƒæ•°æ®ï¼ˆæ ‡é¢˜ã€å±‚çº§ã€çˆ¶èŠ‚ç‚¹ç­‰ï¼‰
    """
    nodes = []
    
    def _traverse(node: dict, parent_id: str = None, path: list = []):
        # ç”ŸæˆèŠ‚ç‚¹ID
        node_id = f"node_{len(nodes)}"
        
        # æ„å»ºå½“å‰è·¯å¾„
        current_path = path + [node.get("title", "")]
        
        # åˆ›å»ºNode
        node_data = {
            "id": node_id,
            "text": node.get("content", ""),
            "metadata": {
                "title": node.get("title", ""),
                "level": node.get("level", 0),
                "parent_id": parent_id,
                "path": " > ".join(current_path),
                "has_children": len(node.get("children", [])) > 0
            }
        }
        
        nodes.append(node_data)
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        for child in node.get("children", []):
            _traverse(child, node_id, current_path)
    
    _traverse(structure)
    return nodes

# endregion
# ============================================


# ============================================
# region ä¸»å‡½æ•°
# ============================================

def parse_markdown_to_nodes(markdown_path: str) -> list:
    """
    è§£æMarkdownæ–‡ä»¶ä¸ºNodeåˆ—è¡¨
    
    å‚æ•°:
        markdown_path: Markdownæ–‡ä»¶è·¯å¾„
    
    è¿”å›:
        Nodeåˆ—è¡¨
    """
    start_time = time.time()
    print(f"\n{'='*50}")
    print(f"ğŸš€ å¼€å§‹è§£ææ–‡æ¡£ç»“æ„")
    print(f"{'='*50}")
    print(f"ğŸ“„ æ–‡ä»¶: {markdown_path}")
    
    # è¯»å–æ–‡ä»¶
    with open(markdown_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"ğŸ“ æ–‡æ¡£é•¿åº¦: {len(content)} å­—ç¬¦")
    
    # AIè§£æç»“æ„
    print(f"\nğŸ¤– è°ƒç”¨AIè§£ææ–‡æ¡£ç»“æ„...")
    structure = parse_document_structure(content)
    
    if not structure:
        print("âŒ æ–‡æ¡£ç»“æ„è§£æå¤±è´¥")
        return []
    
    # ä¿å­˜ç»“æ„ï¼ˆè°ƒè¯•ç”¨ï¼‰
    structure_path = OUTPUT_DIR / f"{Path(markdown_path).stem}_structure.json"
    with open(structure_path, 'w', encoding='utf-8') as f:
        json.dump(structure, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ ç»“æ„å·²ä¿å­˜: {structure_path}")
    
    # è½¬æ¢ä¸ºNodeåˆ—è¡¨
    nodes = structure_to_nodes(structure)
    
    # ä¿å­˜Nodeåˆ—è¡¨
    nodes_path = OUTPUT_DIR / f"{Path(markdown_path).stem}_nodes.json"
    with open(nodes_path, 'w', encoding='utf-8') as f:
        json.dump(nodes, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ Nodeså·²ä¿å­˜: {nodes_path}")
    
    elapsed = time.time() - start_time
    print(f"\n{'='*50}")
    print(f"âœ… è§£æå®Œæˆ!")
    print(f"ğŸ“Š å…±ç”Ÿæˆ {len(nodes)} ä¸ªNode")
    print(f"â±ï¸ è€—æ—¶: {elapsed:.2f} ç§’")
    print(f"{'='*50}")
    
    return nodes

# endregion
# ============================================


if __name__ == "__main__":
    # æµ‹è¯•
    test_file = "output/é‡‡è´­æ–‡ä»¶.md"
    nodes = parse_markdown_to_nodes(test_file)
    
    # æ‰“å°å‰5ä¸ªNodeé¢„è§ˆ
    print("\nğŸ“‹ Nodeé¢„è§ˆï¼ˆå‰5ä¸ªï¼‰:")
    for node in nodes[:5]:
        print(f"\n[{node['id']}] {node['metadata']['title']}")
        print(f"    å±‚çº§: {node['metadata']['level']}")
        print(f"    è·¯å¾„: {node['metadata']['path']}")
        print(f"    å†…å®¹: {node['text'][:100]}..." if node['text'] else "    å†…å®¹: (ç©º)")